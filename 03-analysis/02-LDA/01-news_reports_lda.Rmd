---
title: "nexis_lda"
author: "Hua Tu"
date: "2024-08-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tm)
library(tidytext)
library(quanteda)
library(ldatuning)

library(dplyr)
library(tidyr)
library(readr)
library(tm)
library(quanteda)
library(topicmodels)
library(textmineR)
library(stringr)
library(ggplot2)
library(Matrix)
library(SnowballC)
library(seededlda)
install.packages("parallel")
library(parallel)
```

```{r}
nexis <- read.csv("/Users/iris/Documents/CAPSTONE/nexis/final_nexis_text_date_data/alt5_nexis_body_date.csv")
```

```{r}
# remove NA CleanedText rows in nexis
nexis <- nexis[!is.na(nexis$CleanedText), ]
corp <- corpus(nexis, text_field = "CleanedText")

toks_nexis <- tokens(corp) 
dfm_nexis<- dfm(toks_nexis) %>% 
  dfm_wordstem() 

dtm_nexis <- DocumentTermMatrix(corp)
```



```{r}
# Define a range of topics
k_values <- seq(2, 30, by = 1)

# Initialize a vector to store perplexity values
perplexity_values <- numeric(length(k_values))

# Loop over different numbers of topics
for (i in seq_along(k_values)) {
  lda_model <- LDA(dtm_nexis, k = k_values[i], control = list(seed = 1234))
  perplexity_values[i] <- perplexity(lda_model)
  message(perplexity_values[i])
}

# Create a data frame for plotting
perplexity_df <- data.frame(
  Topics = k_values,
  Perplexity = perplexity_values
)

# Plot the perplexity graph using ggplot2
ggplot(perplexity_df, aes(x = Topics, y = Perplexity)) +
  geom_line() +
  geom_point() +
  labs(title = "Perplexity for LDA model for online discussion on Brexit",
       x = "Number of Topics",
       y = "Perplexity") +
  theme_minimal()
```


```{r}
lda_model_media <- LDA(dtm_nexis, k = 25, control = list(seed = 1234))

saveRDS(lda_model_media, "/Users/iris/Documents/CAPSTONE/final_model_data_save/tmod_lda_nexis_25.rds")
# read the model file
# lda_model_media <- readRDS(file = "/Users/iris/Documents/CAPSTONE/final_model_data_save/tmod_lda_nexis_25.rds")

```

```{r}
beta_matrix <- exp(lda_model_media@beta)
terms <- lda_model_media@terms

# Number of top terms to extract per topic
top_n <- 10

# For each topic, extract the top `n` terms
top_terms <- apply(beta_matrix, 1, function(x) {
  top_indices <- order(x, decreasing = TRUE)[1:top_n]
  terms[top_indices]
})

top_terms[,1]
```

```{r}
gamma_matrix <- lda_model_media@gamma

# Number of top documents to extract per topic
top_n_docs <- 15

# For each topic, find the top `n` documents
top_docs <- apply(gamma_matrix, 2, function(x) {
  top_indices <- order(x, decreasing = TRUE)[1:top_n_docs]
  top_indices
})

# Convert to a list for easier viewing
top_docs_list <- lapply(1:ncol(gamma_matrix), function(i) {
  list(
    topic = i,
    documents = top_docs[, i]
  )
})

# Print the top documents for each topic
for (i in 1:length(top_docs_list)) {
  cat("Top documents for Topic", top_docs_list[[i]]$topic, ":\n")
  print(top_docs_list[[i]]$documents)
  cat("\n")
}
```

```{r}
# randomly select 5 articles from the 15 articles in topic 10
set.seed(1234)
sample_docs <- sample(top_docs_list[[10]][["documents"]], 3)

# Print the text of the selected articles
print("Sampled articles:")
for (i in sample_docs) {
  print(nexis$text[i])
}


for (i in top_docs_list[[10]][["documents"]]) {
  print(nexis$text[i])
}
```

```{r}
# Extract the topic probability matrix (gamma)
topic_probabilities <- lda_model_media@gamma

# Assign each document the most probable topic
most_probable_topic <- apply(topic_probabilities, 1, which.max)

# View the most probable topic for each document
most_probable_topic

# Add the most probable topic to the original data frame
nexis$topic <- most_probable_topic
```

```{r}
write.csv(nexis, file = "/Users/iris/Documents/CAPSTONE/final_model_data_save/nexis_25.csv", row.names = FALSE)
nexis <- read.csv(file = "/Users/iris/Documents/CAPSTONE/final_model_data_save/nexis_25.csv")
```

